{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "caec55e7e1780b0232ffb47ff64caed9eda915d1"
   },
   "source": [
    "## **Data Pre Processing**\n",
    "*  Cleaning the data\n",
    "*  Building word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "847ec12886b0a3bbe7edc2ed9644fe72a91e8d36"
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "input_file = open(\"socialmedia_relevant_cols.csv\", \"r\",encoding='utf-8', errors='replace')\n",
    "\n",
    "# read_csv will turn CSV files into dataframes\n",
    "questions = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfb5962baab7fd0deefcb8943849872d92418fd9"
   },
   "outputs": [],
   "source": [
    "# to clean data\n",
    "def normalise_text (text):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace(r\"\\#\",\"\")\n",
    "    text = text.str.replace(r\"http\\S+\",\"URL\")\n",
    "    text = text.str.replace(r\"@\",\" \")\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \")\n",
    "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a05812772c695c58461339f1f541690219957f28"
   },
   "outputs": [],
   "source": [
    "questions[\"text\"]=normalise_text(questions[\"text\"])\n",
    "#could save to another file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cdb6fbd02954363abfdc45b02fe36323d551cd5"
   },
   "source": [
    "### Embedding using spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e95f2625b8ef75e92c46ea9d4efecd7fbd841841"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "137eaa9eadbf47da7362794a2d0152a4855db04b"
   },
   "outputs": [],
   "source": [
    "doc = questions[\"text\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b4edfb0c82076c73aed61c96aef1813575b817e"
   },
   "outputs": [],
   "source": [
    "questions.choose_one.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b34044aab8c262a4ed79e06dc3727b32c6986141"
   },
   "source": [
    " ### Form 3D vectors for converting to use in CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3bfd291fd8702211a10df94b28823323d53b6d5a"
   },
   "outputs": [],
   "source": [
    "max_sent_len=max(len(doc[i]) for i in range(0,len(doc)))\n",
    "print(max_sent_len)\n",
    "\n",
    "vector_len=len(doc[0][0].vector)\n",
    "print(vector_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cc3fa6ab55e223559dc8976294df2c9e2f75dca"
   },
   "outputs": [],
   "source": [
    "tweet_matrix=np.zeros((len(doc),max_sent_len,vector_len))\n",
    "print(tweet_matrix[0:2,0:3,0:4]) #test print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43fdc67e437da5f6b953fe873bb4ee6cea520803"
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(doc)):\n",
    "    for j in range(0,len(doc[i])):\n",
    "        tweet_matrix[i][j]=doc[i][j].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f7ed876cdc972f12763b5cf5025d18c5c1bba49"
   },
   "source": [
    "### create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "355ff1a4fd5116329c4e4a1fca2327d1cf1de56c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_labels = np.array(questions[\"class_label\"])\n",
    "print(list_labels.shape[0])\n",
    "print(tweet_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35bf578c83baa5a9f6487e376060513d9227bd80"
   },
   "source": [
    "## **CNN in Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71c3f6799ebff0967f700a44e5d71610cc3bf77b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "445a3305a31557ea1c8a535585077754ececb258"
   },
   "outputs": [],
   "source": [
    "#if you need to convert numpy ndarray to tensor explicitely\n",
    "#tweet_matrix = torch.from_numpy(tweet_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8185c2917cbe47d6c7a82f8ebf9cf42a64905273"
   },
   "outputs": [],
   "source": [
    "#for GPU - CUDA\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bbcacb939f52c4f834874c0cbd16a15d9818fce"
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8bad7c21ad03414cba768240890e53a204d67f0a"
   },
   "outputs": [],
   "source": [
    "len_for_split=[int(tweet_matrix.shape[0]/4),int(tweet_matrix.shape[0]*(3/4))]\n",
    "print(len_for_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c87bea754eceb913baa08a0521aaacb906db05b2"
   },
   "outputs": [],
   "source": [
    "test, train=random_split(tweet_matrix,len_for_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "783ebd75c076e4c41437483b9618606b0748c1bf"
   },
   "outputs": [],
   "source": [
    "test.dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "651e4c67928760a49098a5d536b3ecab6ae1314d"
   },
   "source": [
    "### Declare Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0057c657aa1fe65fca683ac97df2cede9a4f7325"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "num_classes = 3\n",
    "learning_rate = 0.001\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "57722b4e99bb779c5ad2a2ff4ba9f137a862cffc"
   },
   "source": [
    "### Load Data\n",
    "\n",
    "The dataset is loaded in batches with the Dataset class and Dataloader Module from torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b9cb3c6733225e159d990aa4cd72359283bfb43"
   },
   "outputs": [],
   "source": [
    "# to transform the data and labels\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56363b83e774e510114012e961496eb1dd2eeb2f"
   },
   "outputs": [],
   "source": [
    "#load labels #truncating total data to keep batch size 100\n",
    "labels_train=list_labels[train.indices[0:8100]]\n",
    "labels_test=list_labels[test.indices[0:2700]]\n",
    "\n",
    "#load train data\n",
    "training_data=train.dataset[train.indices[0:8100]].astype(float)\n",
    "#training_data=training_data.unsqueeze(1)\n",
    "\n",
    "#load test data\n",
    "test_data=test.dataset[test.indices[0:2700]].astype(float)\n",
    "#test_data=test_data.unsqueeze(1)\n",
    "\n",
    "dataset_train = MyDataset(training_data, labels_train)\n",
    "dataset_test = MyDataset(test_data, labels_test)\n",
    "\n",
    "\n",
    "#loading data batchwise\n",
    "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75fdbeccfc961d0b3f93bedab1bb1b86a80ac2a6"
   },
   "source": [
    "### Setting up the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f13fb2fda8769c0bf6b034e5fedf7beb3525545"
   },
   "outputs": [],
   "source": [
    "## setting up the CNN network\n",
    "\n",
    "#arguments(input channel, output channel, kernel size, strides, padding)\n",
    "            \n",
    "            #layer 1 : \n",
    "            # height_out=(h_in-F_h)/S+1=(72-5)/1+1=68\n",
    "            # width_out=(w_in-F_w)/S+1=(384-35)/1+1=350\n",
    "            # no padding given\n",
    "            # height_out=68/2=34 \n",
    "            # width_out=350/5=70\n",
    "            \n",
    "            #layer 2:\n",
    "            # height_out=(h_in-F_h)/S+1=(34-5)/1+1=30\n",
    "            # width_out=(w_in-F_w)/S+1=(70-5)/1+1=66\n",
    "            # height_out=30/2=15 \n",
    "            # width_out=66/2=33\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 15, kernel_size=(5,35), stride=1,padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,5), stride=(2,5)))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(15, 30, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(15 * 33 * 30, 5000)\n",
    "        self.fc2 = nn.Linear(5000, 100)\n",
    "        self.fc3 = nn.Linear(100,3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03e87004b8803019eb1ab08ac8435b01162e69c4"
   },
   "outputs": [],
   "source": [
    "#creating instance of our ConvNet class\n",
    "model = ConvNet()\n",
    "model.to(device) #CNN to GPU\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#CrossEntropyLoss function combines both a SoftMax activation and a cross entropy loss function in the same function\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07072e1e57251e9133e6762f642b42d9f53948d4"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "6e53876c60e0c0d6eae98f35c45417c507c0602c"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = 8100/batch_size\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_t, labels) in enumerate(train_loader):\n",
    "        data_t=data_t.unsqueeze(1)\n",
    "        data_t, labels = data_t.to(device), labels.to(device)\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = model(data_t)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        #print(\"==========forward pass finished==========\")\n",
    "            \n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"==========backward pass finished==========\")\n",
    "        \n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "    \n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct / total) * 100))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80a9ab55e010376688b61321839efdd82ffb3dc9"
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "478ec1890e6542dfbbd05ad24b5fa2ec4e741456"
   },
   "outputs": [],
   "source": [
    "## evaluating model\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data_t, labels in test_loader:\n",
    "        data_t=data_t.unsqueeze(1)\n",
    "        data_t, labels = data_t.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(data_t)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model: {} %'.format((correct / total) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b347c0182fc06e547c25ef2ee3a9f2c35c8e928"
   },
   "source": [
    "### Plot a graph to trace model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "a3299a7f04480735cda7d18a3b5875d2ca3d62d0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "b2fdc188dc5754b2b6898f8c8fb7d5af9d32afb1"
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"runs\")\n",
    "x_len=list(range(len(acc_list)))\n",
    "plt.axis([0, max(x_len), 0, 1])\n",
    "plt.title('result of convNet')\n",
    "loss=np.asarray(loss_list)/max(loss_list)\n",
    "plt.plot(x_len, loss, 'r.', x_len, acc_list, 'b.')\n",
    "plt.show\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
